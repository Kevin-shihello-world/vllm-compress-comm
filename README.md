We utilize compressed communication to compress those Tensors transport during different GPUs in multi-GPU training  So as to save time    And let people without gpus with  nvlink to gain success in ai age. 
To do this, we  self-improved  the training strategy of a new kind of diffusion model called rectified flow  .And let the model can learn from what it failed  in order to make them generation of the Information more stright.
The user may confused why it can generate faster than transporting  those tensors  and achieve the same accuracy. The answer is we utilize inverse fft  To generate those tensors  Faster with  accuracy. 
And the reason of the accuracy also came from  The idea of only a few heads in llm would  do important transformation  .And the Information in hidden states are sparse  So we can compress them without a lot of loss in accuracy
